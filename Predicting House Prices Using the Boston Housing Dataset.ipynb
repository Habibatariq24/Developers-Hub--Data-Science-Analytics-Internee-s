{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:34:17.305138Z","iopub.execute_input":"2025-05-05T16:34:17.305668Z","iopub.status.idle":"2025-05-05T16:34:17.310004Z","shell.execute_reply.started":"2025-05-05T16:34:17.305641Z","shell.execute_reply":"2025-05-05T16:34:17.309194Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Load California Housing Dataset","metadata":{}},{"cell_type":"code","source":"\ncalifornia = fetch_california_housing()\nX = pd.DataFrame(california.data, columns=california.feature_names)\ny = pd.Series(california.target, name='PRICE')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:34:21.180072Z","iopub.execute_input":"2025-05-05T16:34:21.180724Z","iopub.status.idle":"2025-05-05T16:34:21.195155Z","shell.execute_reply.started":"2025-05-05T16:34:21.180689Z","shell.execute_reply":"2025-05-05T16:34:21.194444Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Normalize numerical features","metadata":{}},{"cell_type":"code","source":"\nX = (X - X.mean()) / X.std()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:34:25.051790Z","iopub.execute_input":"2025-05-05T16:34:25.052291Z","iopub.status.idle":"2025-05-05T16:34:25.059973Z","shell.execute_reply.started":"2025-05-05T16:34:25.052266Z","shell.execute_reply":"2025-05-05T16:34:25.059333Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Split the dataset","metadata":{}},{"cell_type":"code","source":"\nX_train, X_test, y_train, y_test = train_test_split(\n    X.values, y.values, test_size=0.2, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:34:28.609670Z","iopub.execute_input":"2025-05-05T16:34:28.610322Z","iopub.status.idle":"2025-05-05T16:34:28.617797Z","shell.execute_reply.started":"2025-05-05T16:34:28.610287Z","shell.execute_reply":"2025-05-05T16:34:28.617032Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Custom Linear Regression Model","metadata":{}},{"cell_type":"code","source":"\nclass LinearRegressionScratch:\n    def fit(self, X, y):\n        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term\n        self.theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n\n    def predict(self, X):\n        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n        return X_b @ self.theta","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:12:36.438415Z","iopub.execute_input":"2025-05-05T16:12:36.439031Z","iopub.status.idle":"2025-05-05T16:12:36.443284Z","shell.execute_reply.started":"2025-05-05T16:12:36.439007Z","shell.execute_reply":"2025-05-05T16:12:36.442574Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Custom Decision Tree and Random Forest","metadata":{}},{"cell_type":"code","source":"\n\nclass DecisionTree:\n    def __init__(self, max_depth=3):\n        self.max_depth = max_depth\n\n    def fit(self, X, y, depth=0):\n        self.feature_index = None\n        self.threshold = None\n        self.left = None\n        self.right = None\n        self.value = np.mean(y)\n\n        if depth >= self.max_depth or len(set(y)) == 1:\n            return\n\n        best_mse = float('inf')\n        for feature in range(X.shape[1]):\n            thresholds = np.unique(X[:, feature])\n            for t in thresholds:\n                left_mask = X[:, feature] <= t\n                right_mask = ~left_mask\n                if sum(left_mask) == 0 or sum(right_mask) == 0:\n                    continue\n                left_y, right_y = y[left_mask], y[right_mask]\n                mse = (left_y.var() * len(left_y) + right_y.var() * len(right_y)) / len(y)\n                if mse < best_mse:\n                    best_mse = mse\n                    self.feature_index = feature\n                    self.threshold = t\n\n        if self.feature_index is not None:\n            left_mask = X[:, self.feature_index] <= self.threshold\n            right_mask = ~left_mask\n            self.left = DecisionTree(self.max_depth)\n            self.right = DecisionTree(self.max_depth)\n            self.left.fit(X[left_mask], y[left_mask], depth + 1)\n            self.right.fit(X[right_mask], y[right_mask], depth + 1)\n\n    def predict_one(self, x):\n        if self.feature_index is None:\n            return self.value\n        if x[self.feature_index] <= self.threshold:\n            return self.left.predict_one(x)\n        else:\n            return self.right.predict_one(x)\n\n    def predict(self, X):\n        return np.array([self.predict_one(x) for x in X])\n\nclass RandomForest:\n    def __init__(self, n_trees=10, max_depth=3):\n        self.n_trees = n_trees\n        self.max_depth = max_depth\n        self.trees = []\n\n    def fit(self, X, y):\n        self.trees = []\n        for _ in range(self.n_trees):\n            idxs = np.random.choice(len(X), len(X), replace=True)\n            tree = DecisionTree(self.max_depth)\n            tree.fit(X[idxs], y[idxs])\n            self.trees.append(tree)\n\n    def predict(self, X):\n        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n        return tree_preds.mean(axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:12:42.075058Z","iopub.execute_input":"2025-05-05T16:12:42.075643Z","iopub.status.idle":"2025-05-05T16:12:42.085885Z","shell.execute_reply.started":"2025-05-05T16:12:42.075615Z","shell.execute_reply":"2025-05-05T16:12:42.084949Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Custom XGBoost Regressor","metadata":{}},{"cell_type":"code","source":"\nclass XGBoostRegressor:\n    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3):\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.max_depth = max_depth\n        self.trees = []\n\n    def fit(self, X, y):\n        self.trees = []\n        pred = np.full(y.shape, np.mean(y))\n        for _ in range(self.n_estimators):\n            residuals = y - pred\n            tree = DecisionTree(self.max_depth)\n            tree.fit(X, residuals)\n            update = tree.predict(X)\n            pred += self.learning_rate * update\n            self.trees.append(tree)\n\n    def predict(self, X):\n        pred = np.zeros(X.shape[0]) + np.mean(y_train)\n        for tree in self.trees:\n            pred += self.learning_rate * tree.predict(X)\n        return pred\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:12:51.152625Z","iopub.execute_input":"2025-05-05T16:12:51.153171Z","iopub.status.idle":"2025-05-05T16:12:51.158275Z","shell.execute_reply.started":"2025-05-05T16:12:51.153146Z","shell.execute_reply":"2025-05-05T16:12:51.157618Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"#  Evaluation Function","metadata":{}},{"cell_type":"code","source":"\ndef evaluate(model, name):\n    preds = model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, preds))\n    r2 = r2_score(y_test, preds)\n    print(f\"{name} RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n    return rmse, r2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:13:00.855091Z","iopub.execute_input":"2025-05-05T16:13:00.855702Z","iopub.status.idle":"2025-05-05T16:13:00.859702Z","shell.execute_reply.started":"2025-05-05T16:13:00.855676Z","shell.execute_reply":"2025-05-05T16:13:00.858997Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Train and Evaluate All Models","metadata":{}},{"cell_type":"code","source":"\n\nprint(\"Training Linear Regression...\")\nlin_reg = LinearRegressionScratch()\nlin_reg.fit(X_train, y_train)\nevaluate(lin_reg, \"Linear Regression\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:13:05.962015Z","iopub.execute_input":"2025-05-05T16:13:05.962252Z","iopub.status.idle":"2025-05-05T16:13:06.025341Z","shell.execute_reply.started":"2025-05-05T16:13:05.962236Z","shell.execute_reply":"2025-05-05T16:13:06.024623Z"}},"outputs":[{"name":"stdout","text":"Training Linear Regression...\nLinear Regression RMSE: 0.7456, R²: 0.5758\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(0.7455813830127764, 0.5757877060324508)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"print(\"\\nTraining Random Forest...\")\nrf = RandomForest(n_trees=10, max_depth=4)\nrf.fit(X_train, y_train)\nevaluate(rf, \"Random Forest\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nTraining XGBoost...\")\nxgb = XGBoostRegressor(n_estimators=10, learning_rate=0.1, max_depth=3)\nxgb.fit(X_train, y_train)\nevaluate(xgb, \"XGBoost\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Feature Importance Visualization","metadata":{}},{"cell_type":"code","source":"def compute_feature_importance(model, feature_names):\n    importance = {name: 0 for name in feature_names}\n\n    def count_splits(tree):\n        if tree.feature_index is not None:\n            importance[feature_names[tree.feature_index]] += 1\n            count_splits(tree.left)\n            count_splits(tree.right)\n\n    for tree in model.trees:\n        count_splits(tree)\n\n    return importance","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_importance(importance_dict, title):\n    items = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n    features, scores = zip(*items)\n    plt.figure(figsize=(10, 5))\n    sns.barplot(x=scores, y=features)\n    plt.title(title)\n    plt.xlabel(\"Importance (split count)\")\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(\"\\nVisualizing Feature Importance...\")\nrf_importance = compute_feature_importance(rf, california.feature_names)\nxgb_importance = compute_feature_importance(xgb, california.feature_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_importance(rf_importance, \"Random Forest Feature Importance\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_importance(xgb_importance, \"XGBoost Feature Importance\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}